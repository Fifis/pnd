% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/step-select.R
\name{gradstep}
\alias{gradstep}
\title{Automatic step selection for numerical derivatives}
\usage{
gradstep(
  FUN,
  x,
  h0 = 1e-05 * (abs(x) + (x == 0)),
  method = c("SW", "CR", "CRm", "DV", "M"),
  control = NULL,
  cores = 2,
  ...
)
}
\arguments{
\item{FUN}{Function for which the optimal numerical derivative step size is needed.}

\item{x}{Numeric vector or scalar: the point at which the derivative is computed
and the optimal step size is estimated.}

\item{h0}{Numeric vector or scalar: initial step size, defaulting to a relative step of
slightly greater than .Machine$double.eps^(1/3) (or absolute step if \code{x == 0}).}

\item{method}{Character indicating the method: \code{"CR"} for \insertCite{curtis1974choice}{pnd},
\code{"CR"} for modified Curtis--Reid, "DV" for \insertCite{dumontet1977determination}{pnd},
\code{"SW"} \insertCite{stepleman1979adaptive}{pnd}, and "M" for
\insertCite{mathur2012analytical}{pnd}.}

\item{control}{A named list of tuning parameters for the method. If \code{NULL},
default values are used. See the documentation for the respective methods. Note that
if \code{control$diagnostics} is \code{TRUE}, full iteration history
including all function evaluations is returned; different methods have
slightly different diagnostic outputs.}

\item{cores}{Integer specifying the number of parallel processes to use. Recommended
value: the number of physical cores on the machine minus one.}

\item{...}{Passed to FUN.}
}
\value{
A list similar to the one returned by \code{optim()} and made of
concatenated individual elements coordinate-wise lists: \code{par} -- the optimal
step sizes found, \code{value} -- the estimated numerical gradient,
\code{counts} -- the number of iterations for each coordinate,
\code{abs.error} -- an estimate of the total approximation error
(sum of truncation and rounding errors),
\code{exitcode} -- an integer code indicating the termination status:
\code{0} indicates optimal termination within tolerance,
\code{1} means that the truncation error (CR method) or the third derivative
(DV method) is zero and large step size is preferred,
\code{2} is returned if there is no change in step size within tolerance,
\code{3} indicates a solution at the boundary of the allowed value range,
\code{4} signals that the maximum number of iterations was reached.
\code{message} -- summary messages of the exit status.
If \code{method.ards$diagnostics} is \code{TRUE}, \code{iterations} is a list of lists
including the full step size search path, argument grids, function values on
those grids, estimated error ratios, and estimated derivative values for
each coordinate.
}
\description{
Automatic step selection for numerical derivatives
}
\details{
We recommend using the Stepleman--Winarsky algorithm because it does not suffer
from over-estimation of the truncation error in the Curtis--Reid approach
and from sensitivity to near-zero third derivatives in the Dumontet--Vignes
approach.
}
\examples{
gradstep(x = 1, FUN = sin, method = "CR")
gradstep(x = 1, FUN = sin, method = "CRm")
gradstep(x = 1, FUN = sin, method = "DV")
gradstep(x = 1, FUN = sin, method = "SW")
gradstep(x = 1, FUN = sin, method = "M")
# Works for gradients
gradstep(x = 1:4, FUN = function(x) sum(sin(x)))
}
\references{
\insertAllCited{}
}
\seealso{
\code{\link[=step.CR]{step.CR()}} for Curtis--Reid (1974) and its modification,
\code{\link[=step.DV]{step.DV()}} for Dumontet--Vignes (1977),
\code{\link[=step.SW]{step.SW()}} for Stepleman--Winarsky (1979), and
\code{\link[=step.M]{step.M()}} for Mathur (2012).
}
