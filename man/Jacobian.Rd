% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gradient.R
\name{Jacobian}
\alias{Jacobian}
\title{Jacobian matrix computation with parallel capabilities}
\usage{
Jacobian(
  FUN,
  x,
  deriv.order = 1L,
  side = 0,
  acc.order = 2,
  h = NULL,
  h0 = NULL,
  control = list(),
  f0 = NULL,
  cores = 1,
  preschedule = TRUE,
  func = NULL,
  report = 1L,
  ...
)
}
\arguments{
\item{FUN}{A function returning a numeric scalar or a vector whose derivatives are to be
computed. If the function returns a vector, the output will be a Jacobian.}

\item{x}{Numeric vector or scalar: the point(s) at which the derivative is estimated.
\code{FUN(x)} must be finite.}

\item{deriv.order}{Integer or vector of integers indicating the desired derivative order,
\eqn{\mathrm{d}^m / \mathrm{d}x^m}{d^m/dx^m}, for each element of \code{x}.}

\item{side}{Integer scalar or vector indicating the type of finite difference:
\code{0} for central, \code{1} for forward, and \code{-1} for backward differences.
Central differences are recommended unless computational cost is prohibitive.}

\item{acc.order}{Integer or vector of integers specifying the desired accuracy order
for each element of \code{x}.
The final error will be of the order \eqn{O(h^{\mathrm{acc.order}})}{O(h^acc.order)}.}

\item{h}{Numeric or character specifying the step size(s) for the numerical
difference or a method of automatic step determination (\code{"CR"}, \code{"CRm"},
\code{"DV"}, or \code{"SW"} to be used in \code{\link[=gradstep]{gradstep()}}).}

\item{h0}{Numeric scalar of vector: initial step size for automatic search with
\code{gradstep()}.}

\item{control}{A named list of tuning parameters passed to \code{gradstep()}.}

\item{f0}{Optional numeric: if provided, used to determine the vectorisation type
to save time. If FUN(x) must be revaluated (e.g. second derivatives), saves one evaluation.}

\item{cores}{Integer specifying the number of CPU cores used for parallel computation.
Recommended to be set to the number of physical cores on the machine minus one.}

\item{preschedule}{Logical: if \code{TRUE}, disables pre-scheduling for \code{mclapply()}
or enables load balancing with \code{parLapplyLB()}. Recommended for functions that
take less than 0.1 s per evaluation.}

\item{func}{For compatibility with \code{numDeriv::grad()} only. If instead of
\code{FUN}, \code{func} is used, it will be reassigned to \code{FUN} with a warning.}

\item{report}{Integer for the level of detail in the output. If \code{0},
returns a gradient without any attributes; if \code{1},
attaches the step size and its selection method: \code{2} or higher attaches the full
diagnostic output as an attribute.}

\item{...}{Additional arguments passed to \code{FUN}.}
}
\value{
Matrix where each row corresponds to a function output and each column
to an input coordinate. For scalar-valued functions, a warning is issued and
the output is returned as a row matrix.
}
\description{
Computes the numerical Jacobian for vector-valued functions. Its columns are
partial derivatives of the function with respect to the input elements.
This function supports both two-sided (central, symmetric) and
one-sided (forward or backward) derivatives. It can utilise parallel processing
to accelerate computation of gradients for slow functions or
to attain higher accuracy faster. Currently, only Mac and Linux are supported
\code{parallel::mclapply()}. Windows support with \code{parallel::parLapply()}
is under development.
}
\examples{
\dontrun{
slowFun <- function(x) {Sys.sleep(0.05); print(x, digits = 12); sum(sin(x))}
slowFunVec <- function(x) {Sys.sleep(0.05); print(x, digits = 12)
                           c(sin = sum(sin(x)), exp = sum(exp(x)))}
true.g <- cos(1:4)  # Analytical gradient
true.j <- rbind(cos(1:4), exp(1:4)) # Analytical Jacobian
x0 <- c(each = 1, par = 2, is = 3, named = 4)

# Compare computation times
system.time(g.slow <- numDeriv::grad(slowFun, x = x0) - true.g)
system.time(j.slow <- numDeriv::jacobian(slowFunVec, x = x0) - true.j)
system.time(g.fast <- Grad(slowFun, x = x0, cores = 4) - true.g)
system.time(j.fast <- Grad(slowFunVec, x = x0, cores = 4) - true.j)
system.time(j.fast4 <- Grad(slowFunVec, x = x0, acc.order = 4, cores = 4) - true.j)

# Compare accuracy
rownames(j.slow) <- rep("numDeriv.jac", nrow(j.slow))
rownames(j.fast) <- paste0("pnd.jac.order2.", rownames(j.fast))
rownames(j.fast4) <- paste0("pnd.jac.order.4", rownames(j.fast4))
# Discrepancy
print(rbind(numDeriv.grad = g.slow, pnd.Grad = g.fast, j.slow, j.fast, j.fast4), 2)
# The order-4 derivative is more accurate for functions
# with non-zero third and higher derivatives -- look at pnd.jac.order.4
}

}
\seealso{
\code{\link[=GenD]{GenD()}}, \code{\link[=Grad]{Grad()}}
}
