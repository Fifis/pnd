% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/finite-diff.R
\name{Grad}
\alias{Grad}
\title{Parallelised gradient computation}
\usage{
Grad(
  func,
  x,
  side = c("central", "forward", "backward"),
  acc.order = if (side[1] == "central") 2 else 1,
  h = .Machine$double.eps^(1/(1 + acc.order)),
  parallel = c("auto", "fork", "PSOCK", "none"),
  cores = 2,
  cluster = NULL,
  load.balance = TRUE,
  ...
)
}
\arguments{
\item{func}{A function that returns a numeric scalar or a vector. If the function is vector-valued, the, the result is the Jacobian.}

\item{x}{A point at which the gradient or Jacobian needs to be estimated.}

\item{side}{Passed to \code{fdCoef()}. Centred or one-sided differences. Unless the function is computationally prohibitively expensive, two-sided differences are strongly recommended.}

\item{acc.order}{Desired order of accuracy. The error is usually O(h^acc.order). To achieve this order of accuracy, the function needs to be evaluated \code{acc.order*length(x)} times.}

\item{h}{The numerical difference step size. Too large = the slope of the secant is a bad estimator of the gradient, too small = ill conditioning (0/0).}

\item{parallel}{If TRUE, estimates the gradient via finite differences where the function is evaluated in parallel.}

\item{cores}{Number of forked processes.}

\item{cluster}{A cluster on which the computations are done.}

\item{load.balance}{If TRUE, disables pre-scheduling for \code{mclapply} or enables load balancing via \code{parLapplyLB}.}

\item{...}{Passed to \code{func}.

Note that for one-sided problems, the step size that make the formula error
equal to the truncation error is of the order Mach.eps^(1/2) and for two-sided, Mach.eps^(1/3).
However, the optimal step size depends on the value of the higher-order derivatives
that is not available in general (or required extra computation that is, in turn, prone to numerical error).}
}
\value{
If \code{func} returns a scalar, a vector of the same length as \code{x}.
If \code{func} returns a vector, then, a matrix of dimensions \code{length(f(x)) length(x)}
}
\description{
Computes a two- or one-sided numerical derivative that approximates the gradient | Jacobian using the indicated number of cores for maximum efficiency.
}
\examples{
\dontrun{
slowFunScalar <- function(x) {Sys.sleep(0.04); print(x, digits = 12); sum(sin(x))}
slowFunVector <- function(x) {Sys.sleep(0.04); print(x, digits = 12); c(sum(sin(x)), sum(exp(x)))}
true.g <- cos(1:4) # Analytical gradient
true.j <- rbind(cos(1:4), exp(1:4)) # Analytical Jacobian
system.time(g.slow <- numDeriv::grad(slowFunScalar, x = 1:4) - true.g)
system.time(j.slow <- numDeriv::jacobian(slowFunVector, x = 1:4) - true.j)
system.time(g.fast <- Grad(slowFunScalar, x = 1:4,
                                   parallel = TRUE, cores = 4) - true.g)
system.time(j.fast <- Grad(slowFunVector, x = 1:4,
                                   parallel = TRUE, cores = 4) - true.j)
system.time(j.fast4 <- Grad(slowFunVector, x = 1:4, order = 4,
                                    parallel = TRUE, cores = 4) - true.j)
rownames(j.slow) <- c("numDeriv.jacobian", "")
rownames(j.fast) <- c("fast.jacobian.order2", "")
rownames(j.fast4) <- c("fast.jacobian.order4", "")
# Discrepancy
rbind(numDeriv.grad = g.slow, fast.grad = g.fast, j.slow, j.fast, j.fast4)
# The order-4 derivative is more accurate for functions with large high-order derivatives
}

}
