% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gradient.R
\name{Grad}
\alias{Grad}
\title{Gradient computation with parallel capabilities}
\usage{
Grad(
  FUN,
  x,
  deriv.order = 1L,
  side = 0,
  acc.order = 2,
  h = (abs(x) + (x == 0)) * .Machine$double.eps^(1/(deriv.order + acc.order)),
  h0 = NULL,
  control = list(),
  f0 = NULL,
  cores = 1,
  load.balance = TRUE,
  func = NULL,
  report = 1L,
  ...
)
}
\arguments{
\item{FUN}{A function returning a numeric scalar or a vector whose derivatives are to be computed
If the function returns a vector, the output will be is a Jacobian.}

\item{x}{Numeric vector or scalar: the point(s) at which the derivative is estimated.
\code{FUN(x)} must be finite value.}

\item{deriv.order}{Integer or vector of integers indicating the desired derivative order,
\eqn{\mathrm{d}^m / \mathrm{d}x^m}{d^m/dx^m}, for each element of \code{x}.}

\item{side}{Integer scalar or vector indicating the type of finite difference:
\code{0} for central, \code{1} for forward, and \code{-1} for backward differences.
Central differences are recommended unless computational cost is prohibitive.}

\item{acc.order}{Integer or vector of integers specifying the desired accuracy order
for each element of \code{x}.
The final error will be of the order \eqn{O(h^{\mathrm{acc.order}})}{O(h^acc.order)}.}

\item{h}{Numeric or character specifying the step size(s) for the numerical
difference or a method of automatic step determination (\code{"CR"}, \code{"CRm"},
\code{"DV"}, or \code{"SW"} to be used in \code{\link[=gradstep]{gradstep()}}).}

\item{h0}{Numeric scalar of vector: initial step size for automatic search with
\code{gradstep()}.}

\item{control}{A named list of tuning parameters passed to \code{gradstep()}.}

\item{f0}{Optional numeric: if provided and applicable, used
where the stencil contains zero (i.e. \code{FUN(x)} is part of the sum)
to save time.
TODO: Currently ignored.}

\item{cores}{Integer specifying the number of CPU cores used for parallel computation.
Recommended to be set to the number of physical cores on the machine minus one.}

\item{load.balance}{Logical: if \code{TRUE}, disables pre-scheduling for \code{mclapply()}
or enables load balancing with \code{parLapplyLB()}.}

\item{func}{For compatibility with \code{numDeriv::grad()} only. If instead of
\code{FUN}, \code{func} is used, it will be reassigned to \code{FUN} with a warning.}

\item{report}{Integer for the level of detail in the output. If \code{0},
returns a gradient without any attributes; if \code{1},
attaches the step size and its selection method: \code{2} or higher attaches the full
diagnostic output as an attribute.}

\item{...}{Additional arguments passed to \code{FUN}.}
}
\value{
Numeric vector of the gradient. If \code{FUN} returns a vector,
a warning is issued suggesting the use of \code{Jacobian()}.
}
\description{
Computes numerical derivatives and gradients of scalar-valued functions using
finite differences. This function supports both two-sided (central, symmetric) and
one-sided (forward or backward) derivatives. It can utilise parallel processing
to accelerate computation of gradients for slow functions or
to attain higher accuracy faster. Currently, only Mac and Linux are supported
\code{parallel::mclapply()}. Windows support with \code{parallel::parLapply()}
is under development.
}
\examples{
f <- function(x) sum(sin(x))
g1 <- Grad(FUN = f, x = 1:4)
g2 <- Grad(FUN = f, x = 1:4, h = 7e-6)
g2 - g1  # Tiny differences due to different step sizes
g.auto <- Grad(FUN = f, x = 1:4, h = "SW")
g3.full <- Grad(FUN = f, x = 1:4, h = "SW", report = 2)
print(g3.full)
attr(g3.full, "step.search")$exitcode  # Success

}
\seealso{
\code{\link[=GenD]{GenD()}}, \code{\link[=Jacobian]{Jacobian()}}
}
