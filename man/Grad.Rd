% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gradient.R
\name{Grad}
\alias{Grad}
\title{Gradient and Jacobian computation with parallel capabilities}
\usage{
Grad(
  FUN,
  x,
  deriv.order = 1L,
  side = 0,
  acc.order = ifelse(abs(side) == 1, 1L, 2L),
  h = .Machine$double.eps^(1/(deriv.order + acc.order)),
  f0 = NULL,
  parallel = c("auto", "fork", "PSOCK", "none"),
  cores = 2,
  cluster = NULL,
  load.balance = TRUE,
  func = NULL,
  ...
)
}
\arguments{
\item{FUN}{A function returning a numeric scalar or a vector.
If the function returns a vector, the output will be is a Jacobian.
If instead of \code{FUN}, \code{func} is passed, as in \code{numDeriv::grad},
it will be reassigned to \code{FUN} with a warning.}

\item{x}{Numeric vector or scalar: point at which the derivative is estimated.
\code{FUN(x)} must return a finite value.}

\item{deriv.order}{Integer indicating the derivative order,
\eqn{\mathrm{d}^m / \mathrm{d}x^m}{d^m/dx^m}.}

\item{side}{Integer scalar or vector indicating difference type:
\code{0} for central, \code{1} for forward, and \code{-1} for backward differences.
Using \code{2} (for 'two-sided') triggers a warning and is treated as \code{0}.
with a warning.
Central differences are recommended unless computational cost is prohibitive.}

\item{acc.order}{Integer specifying the desired accuracy order.
The error typically scales as \eqn{O(h^{\mathrm{acc.order}})}{O(h^acc.order)}.}

\item{h}{Numeric scalar or vector specifying the step size for the numerical
difference. Must be length 1 or match \code{length(x)}.
If too large, the slope of the secant poorly estimates the derivative;
if too small, it leads to numerical instability due to the function value rounding.}

\item{f0}{Optional numeric scalar or vector: if provided and applicable, used
where the stencil contains zero (i.e. \code{FUN(x)} is part of the sum)
to save time. Currently ignored.}

\item{parallel}{String indicating the parallel execution mode.
This argument is ignored if a valid \code{cluster} is provided,
as the cluster settings take precedence.
On Mac and Linux, use \code{"fork"} to invoke \code{mclapply} or \code{"PSOCK"}
to create a PSOCK cluster dynamically.
On Windows, use \code{"PSOCK"} if you have not created the cluster yet, although
using a dedicated cluster via \code{cluster} is highly recommended to reduce overhead.
\code{"auto"} switches to \code{"PSOCK"} on Windows (with a warning)
and to \code{"fork"} on everything else.}

\item{cores}{Integer specifying the number of parallel processes to use.}

\item{cluster}{An optional cluster object. Ensure that necessary objects and packages
are available in the cluster environment with \code{clusterExport()} and \code{clusterEvalQ()}.}

\item{load.balance}{Logical: if \code{TRUE}, disables pre-scheduling for \code{mclapply()}
or enables load balancing with \code{parLapplyLB()}.}

\item{func}{Deprecated; for \code{numDeriv::grad()} compatibility only.}

\item{...}{Additional arguments passed to \code{FUN}.}
}
\value{
Depends on the output of \code{FUN}. If \code{FUN} returns a scalar:
returns a gradient vector matching the length of \code{x}. If \code{FUN} returns a vector:
returns a Jacobian matrix with dimensions \code{length(FUN(x)), length(x)}.
Unlike the output of \code{numDeriv::grad} and \code{numDeriv::jacobian},
this output preserves the names of \code{x} and \code{FUN(x)}.
}
\description{
Computes numerical derivatives, gradients and Jacobians. Supports both two-sided
(central) and one-sided (forward or backward) derivatives. Calculations can be
executed on multiple cores to cut down execution time for slow functions or
to attain higher accuracy faster.
}
\details{
The optimal step size for one-sided differences typically approaches Mach.eps^(1/2)
to balance the Taylor series truncation error with the rounding error due to storing
function values with limited precision. For two-sided differences, it is proportional
to Mach.eps^(1/3). However, selecting the best step size typically requires knowledge
of higher-order derivatives, which may not be readily available. Future releases
will allow character arguments to invoke automatic data-driven step-size selection.

The use of \code{f0} can reduce computation time similar to the use of \code{f.lower}
and \code{f.upper} in \code{uniroot()}.
}
\examples{
\dontrun{
slowFun <- function(x) {Sys.sleep(0.05); print(x, digits = 12); sum(sin(x))}
slowFunVec <- function(x) {Sys.sleep(0.05); print(x, digits = 12)
                           c(sin = sum(sin(x)), exp = sum(exp(x)))}
true.g <- cos(1:4)  # Analytical gradient
true.j <- rbind(cos(1:4), exp(1:4)) # Analytical Jacobian
x0 <- c(each = 1, par = 2, is = 3, named = 4)

# Compare computation times
system.time(g.slow <- numDeriv::grad(slowFun, x = x0) - true.g)
system.time(j.slow <- numDeriv::jacobian(slowFunVec, x = x0) - true.j)
system.time(g.fast <- Grad(slowFun, x = x0, cores = 4) - true.g)
system.time(j.fast <- Grad(slowFunVec, x = x0, cores = 4) - true.j)
system.time(j.fast4 <- Grad(slowFunVec, x = x0, acc.order = 4, cores = 4) - true.j)

# Compare accuracy
rownames(j.slow) <- rep("numDeriv.jac", nrow(j.slow))
rownames(j.fast) <- paste0("pnd.jac.order2.", rownames(j.fast))
rownames(j.fast4) <- paste0("pnd.jac.order.4", rownames(j.fast4))
# Discrepancy
print(rbind(numDeriv.grad = g.slow, pnd.Grad = g.fast, j.slow, j.fast, j.fast4), 2)
# The order-4 derivative is more accurate for functions
# with non-zero third and higher derivatives -- look at pnd.jac.order.4
}

}
