% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gradient.R
\name{Grad}
\alias{Grad}
\title{Gradient and Jacobian computation with parallel capabilities}
\usage{
Grad(
  FUN,
  x,
  deriv.order = 1L,
  side = 0,
  acc.order = ifelse(abs(side) == 1, 1L, 2L),
  h = (abs(x) + (x == 0)) * .Machine$double.eps^(1/(deriv.order + acc.order)),
  h0 = NULL,
  method.args = list(),
  f0 = NULL,
  cores = 2,
  load.balance = TRUE,
  func = NULL,
  report = 1L,
  ...
)
}
\arguments{
\item{FUN}{A function returning a numeric scalar or a vector.
If the function returns a vector, the output will be is a Jacobian.
If instead of \code{FUN}, \code{func} is passed, as in \code{numDeriv::grad},
it will be reassigned to \code{FUN} with a warning.}

\item{x}{Numeric vector or scalar: point at which the derivative is estimated.
\code{FUN(x)} must return a finite value.}

\item{deriv.order}{Integer indicating the derivative order,
\eqn{\mathrm{d}^m / \mathrm{d}x^m}{d^m/dx^m}.}

\item{side}{Integer scalar or vector indicating difference type:
\code{0} for central, \code{1} for forward, and \code{-1} for backward differences.
Central differences are recommended unless computational cost is prohibitive.}

\item{acc.order}{Integer specifying the desired accuracy order.
The error typically scales as \eqn{O(h^{\mathrm{acc.order}})}{O(h^acc.order)}.}

\item{h}{Numeric scalar, vector, or character specifying the step size for the numerical
difference. If character (\code{"CR"}, \code{"CRm"}, \code{"DV"}, or \code{"SW"}),
calls \code{gradstep()} with the appropriate step-selection method.
Must be length 1 or match \code{length(x)}.}

\item{h0}{Numeric scalar of vector: initial step size for automatic search with
\code{gradstep()}.}

\item{method.args}{A named list of tuning parameters passed to \code{gradstep()}.}

\item{f0}{Optional numeric scalar or vector: if provided and applicable, used
where the stencil contains zero (i.e. \code{FUN(x)} is part of the sum)
to save time. Currently ignored.}

\item{cores}{Integer specifying the number of parallel processes to use.}

\item{load.balance}{Logical: if \code{TRUE}, disables pre-scheduling for \code{mclapply()}
or enables load balancing with \code{parLapplyLB()}.}

\item{func}{Deprecated; for \code{numDeriv::grad()} compatibility only.}

\item{report}{Integer: if \code{0}, returns a gradient without any attributes; if \code{1},
attaches the step size and its selection method: \code{2} or higher, attaches the full
diagnostic output (overrides \code{diagnostics = FALSE} in \code{method.args}).}

\item{...}{Additional arguments passed to \code{FUN}.}
}
\value{
Depends on the output of \code{FUN}. If \code{FUN} returns a scalar:
returns a gradient vector matching the length of \code{x}. If \code{FUN} returns a vector:
returns a Jacobian matrix with dimensions \code{length(FUN(x)), length(x)}.
Unlike the output of \code{numDeriv::grad} and \code{numDeriv::jacobian},
this output preserves the names of \code{x} and \code{FUN(x)}.
}
\description{
Computes numerical derivatives, gradients and Jacobians. Supports both two-sided
(central) and one-sided (forward or backward) derivatives. Calculations can be
executed on multiple cores to cut down execution time for slow functions or
to attain higher accuracy faster. Currently for Mac & Linux only because
Windows cannot handle \code{parallel::mclapply()}. A \code{parallel::parLapply}
version is in development.
}
\details{
If the step size is too large, the slope of the secant poorly estimates the derivative;
if it is too small, it leads to numerical instability due to the function value rounding.

The optimal step size for one-sided differences typically approaches Mach.eps^(1/2)
to balance the Taylor series truncation error with the rounding error due to storing
function values with limited precision. For two-sided differences, it is proportional
to Mach.eps^(1/3). However, selecting the best step size typically requires knowledge
of higher-order derivatives, which may not be readily available. Future releases
will allow character arguments to invoke automatic data-driven step-size selection.

The use of \code{f0} can reduce computation time similar to the use of \code{f.lower}
and \code{f.upper} in \code{uniroot()}.
}
\examples{

f <- function(x) sum(sin(x))
g1 <- Grad(x = 1:4, FUN = f)
g2 <- Grad(x = 1:4, FUN = f, h = 7e-6)
g2 - g1 # Tiny differences due to different step sizes
g3 <- Grad(x = 1:4, FUN = f, h = "SW")
g3.full <- Grad(x = 1:4, FUN = f, h = "SW", report = 2)
attr(g3.full, "step.search")$exitcode  # Success

\dontrun{
slowFun <- function(x) {Sys.sleep(0.05); print(x, digits = 12); sum(sin(x))}
slowFunVec <- function(x) {Sys.sleep(0.05); print(x, digits = 12)
                           c(sin = sum(sin(x)), exp = sum(exp(x)))}
true.g <- cos(1:4)  # Analytical gradient
true.j <- rbind(cos(1:4), exp(1:4)) # Analytical Jacobian
x0 <- c(each = 1, par = 2, is = 3, named = 4)

# Compare computation times
system.time(g.slow <- numDeriv::grad(slowFun, x = x0) - true.g)
system.time(j.slow <- numDeriv::jacobian(slowFunVec, x = x0) - true.j)
system.time(g.fast <- Grad(slowFun, x = x0, cores = 4) - true.g)
system.time(j.fast <- Grad(slowFunVec, x = x0, cores = 4) - true.j)
system.time(j.fast4 <- Grad(slowFunVec, x = x0, acc.order = 4, cores = 4) - true.j)

# Compare accuracy
rownames(j.slow) <- rep("numDeriv.jac", nrow(j.slow))
rownames(j.fast) <- paste0("pnd.jac.order2.", rownames(j.fast))
rownames(j.fast4) <- paste0("pnd.jac.order.4", rownames(j.fast4))
# Discrepancy
print(rbind(numDeriv.grad = g.slow, pnd.Grad = g.fast, j.slow, j.fast, j.fast4), 2)
# The order-4 derivative is more accurate for functions
# with non-zero third and higher derivatives -- look at pnd.jac.order.4
}

}
\seealso{
\code{\link[=gradstep]{gradstep()}} for automatic step-size selection.
}
